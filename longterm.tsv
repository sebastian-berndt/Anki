Assoziativ	Eine zweistellige Verknuepfung \( \ast : A \times A \rightarrow A\) auf einer Menge \(A\) hei\ss t assoziativ, wenn fuer alle \(a,b,c \in A\) das Assoziativgesetz \[a \ast (b \ast c) = (a \ast b) \ast c\] gilt.
Verknuepfung	Fuer eine natuerliche Zahl \(n \in \mathbb{N}\) seien \(n\) Mengen \( A_1, \cdots, A_n\) und eine weitere Menge \(B\) gegeben. Dann bezeichnet man eine Abbildung \( \ast\) des kartesischen Produkts \(A_1 \times \cdots \times A_n\) nach \(B\) als \(n\)-stellige Verknuepfung. \[ \ast : A_1 \times \cdots \times A_n \rightarrow B, (a_1,\ldots,a_n) \mapsto \ast (a_1, \ldots, a_n)\] Anstatt \(\ast (a_1, \ldots, a_n)\) schreibt man meistens \( a_1 \ast \ldots \ast a_n\)
Innere Verknuepfung	Seien \(A_1,\ldots,A_n\) und \(B\) Mengen. Ist dann \(*\) eine \(n\)-stellige Verknuepfung mit \[*: A_1,\ldots,A_n \rightarrow B\] mit \(A_i = B\) fuer alle \(1 \leq i \leq n\), so nennt man \(*\) innere \(n\)-stellige Verknuepfung.
Au\ss ere Verknuepfung	Seien \(A_1,\ldots,A_n\) und \(B\) Mengen. Ist dann \(*\) eine \(n\)-stellige Verknuepfung mit \[*: A_1,\ldots,A_n \rightarrow B\] mit \(A_i \neq B\) fuer \(1 \leq i \leq m\) und \(A_i = B\) fuer \(m+1 \leq i \leq n\) fuer ein \(M\) mit \(0 \leq m < n\), so nennt man \(*\) aeu\ss ere \(n\)-stellige Verknuepfung.
Magma/Gruppoid	Ein Magma (oder auch Gruppoid) ist eine Algebraische Struktur \((M,*)\) bestehend aus einer Menge \(M\) und einer inneren, zweistelligen Verknuepfung \[*:M \times M \rightarrow M.\]
Halbgruppe	Ein assoziatives Magma hei\ss t Halbgruppe.
Monoid	Ein Monoid ist ein Halbgruppe mit neutralem Element.
Neutrales Element	Sei \((S,\ast)\) ein Magma. Dann hei\ss t ein Element \(e \in S\) \begin{description} \item[linksneutral], falls \(e \ast a = a\) fuer alle \(a \in S\) ist, \item[rechtsneutral], falls \(a \ast e = a\) fuer alle \(a \in S\) ist, \item[neutral], falls \(e\) linksneutral und rechtsneutral ist. \end{description}
Inverses Element	Sei \((S,\ast)\) ein Magma und \(e\) ein neutrales Element von \((S,\ast)\). Dann hei\ss t ein Element \(a \in S\) \begin{description} \item[Linksinverse von b] falls \(a \ast b = e\) fuer \(b \in S\) ist, \item[Rechtsinverse von b] falls \(b \ast a = e\) fuer \(b \in S\) ist, \item[Inverse von b] falls \(a\) sowohl die Linksinverse als auch die Rechtsinverse von b ist. \end{description}
Gruppe	Eine Gruppe ist eine Algebraische Struktur bestehend aus einer Menge \(G\) mit einer inneren zweistelligen Verknuepfung fuer die gilt: \begin{enumerate} \item Die Verknuepfung ist assoziativ \item Es gibt ein neutrales Element \(e \in G\) \item Zu jedem \(a \in G\) gibt es ein inverses Element \end{enumerate}
Kommutative Gruppe	Eine Gruppe \((G,\oplus)\) hei\ss t kommutativ oder abelsch, falls fuer alle \(a,b \in G\) das Kommutativgesetz gilt: \[ a \oplus b = b \oplus a.\]
Untergruppe	Sei \((G,\oplus)\) eine Gruppe und \(H \subseteq G\). Ist \((H,\oplus)\) eine Gruppe, so nennt man \((H, \oplus)\) eine Untergruppe von \((G,\oplus)\).
Gruppenhomomorphismus	Seien \((G_1, \oplus)\) und \((G_2, \otimes)\) Gruppen. Eine Abbildung \[ \varphi : G_1 \rightarrow G_2, g \mapsto \varphi (g),\] hei\ss t Gruppenhomomorphismus, wenn \[\varphi(a \oplus b) = \varphi (a) \otimes \varphi (b), \forall a,b \in G_1\] gilt.
Gruppenisomorphismus	Ein bijektiver Gruppenhomomorphismus wird Gruppenisomorphismus genannt.
Ring	Ein Ring ist eine eine Algebraische Struktur bestehend aus einer Menge \(R\) mit zwei zweistelligen Verknuepfungen \( \oplus, \otimes\) fuer die gilt: \begin{enumerate} \item \((R,\oplus)\) ist eine kommutative Gruppe \item \((R,\otimes)\) ist eine Halbgruppe \item Fuer alle \(a,b,c \in R\) gelten die Distributivgesetze mit: \[a \otimes (b \oplus c) = a \otimes b \oplus a \otimes c\] \[(a \oplus b) \otimes c = a \otimes c \oplus b \otimes c\]  \end{enumerate}
Kommutativer Ring	Ein Ring \((R,\oplus, \otimes)\) hei\ss t kommutativ falls fuer alle \(a,b \in R\) mit \(\otimes\) das Kommutativgesetz gilt: \[a \otimes b = b \otimes a\]
Einselement	Sei \((R,\oplus, \otimes)\) ein Ring. Ein Element \(1 \in R\) hei\ss t Einselement (kurz Eins), falls die Halbgruppe \((R,\otimes)\) ein Monoid ist, also ein neutrales Element 1 besitzt.
Ring mit Eins	Sei \((R,\oplus, \otimes)\) ein Ring. Ist die Halbgruppe \((R,\otimes)\) ein Monoid, besitzt also ein Einselement, dann nennt man \((R, \oplus, \otimes)\) einen Ring mit Eins.
Koerper	Ein Koerper ist eine Algebraische Struktur bestehend aus einer Menge \(K\) mit zwei inneren zweistelligen Verknuepfungen \(\oplus\) und \(\otimes\) fuer die folgende Regeln erfuellt sind: \begin{enumerate} \item \((K,\oplus)\) ist eine kommutative Gruppe. (Dabei hei\ss t das neutrale Element bzgl \(\oplus\) Null und wird mit \(0\) bezeichnet. Das zu \(a \in K\) inverse Element bezeichnet man mit \(-a\).) \item \((K \setminus \{0\},\otimes)\) ist eine kommutative Gruppe. (Man nennt das neutrale Element bzgl. \(\otimes\) Eins und bezeichnet es mit \(1\). Das zu \(a \in K \setminus \{0\}\) inverse Element bezeichnet man mit \(a^{-1}\)) \item Es gelten die Distributivgesetze, sodass fuer alle \(a,b,c \in K\) gilt: \[a \otimes (b \oplus c) = a \otimes b \oplus a \otimes c,\] \[(a \oplus b) \otimes c = a \otimes c \oplus b \otimes c.\] \end{enumerate}
Nullteiler	Sei \((R,\oplus,\otimes)\) ein Ring. Dann hei\ss t \(a \in R\) ein Nullteiler, wenn ein \( b \in R \setminus \{0\}\) mit \(a \otimes b = 0\) existiert.
Teilkoerper	Sei \((K, \oplus, \otimes)\) ein Koerper und \(L \subseteq K\). Ist \((L, \oplus, \otimes)\) ein Koerper, so nennen wir diesen einen Teilkoerper von \((K,\oplus,\otimes).\)

Matrix-Multiplikation	Sei \((R,+,\cdot)\) ein kommutativer Ring mit Eins. Die Multiplikation zweier Matrizen ist definiert als: \[*:R^{n,m} \times R^{m,s} \rightarrow R^{n,s}, \quad (A,B) \mapsto A*B=[c_{ij}], \quad c_{ij}:=\sum_{k=1}^m a_{ik}b_{bj}.\]
Skalare Matrix-Multiplikation	Sei \((R,+,\cdot)\) ein kommutativer Ring mit Eins. Die Multiplikation eines Skalars und einer Matrix ist definiert als: \[\cdot: R \times R^{n,m} \rightarrow R^{n,m}, \quad (\lambda, A) \mapsto \lambda \cdot A:=[\lambda a_{ij}].\]


Vektorraum	Sei \(K\) ein Koerper. Ein Vektorraum ueber \(K\) (kurz: \(K\)-Vektorraum) ist eine Menge \(V\) mit zwei Abbildungen, \begin{description} \item[(Addition)] \(+:V \times V \rightarrow V, \quad (v,w) \mapsto v+w,\) \item[(Skalare Multiplikation)] \(\cdot :K \times V \rightarrow V, \quad (\lambda,v) \mapsto \lambda \cdot v,\)\end{description} fuer die folgende Regeln erfuellt sind: \begin{enumerate} \item \((V,+)\) ist eine kommutative Gruppe. \item Fuer alle \(v,w \in V\) und \(\lambda, \mu \in K\) gelten: \begin{enumerate} \item \(\lambda \cdot (\mu \cdot v)=(\lambda \mu) \cdot v.\) \item \(1 \cdot v = v\) \item \(\lambda \cdot (v+w)=\lambda \cdot v + \lambda \cdot w.\) \item \((\lambda + \mu) \cdot v = \lambda \cdot v + \mu \cdot v.\) \end{enumerate}\end{enumerate}
Unterraum	Sei \((V,+,\cdot)\) ein \(K\)-Vektorraum und sei \(U \subseteq V.\) Ist \((U,+,\cdot)\) ein \(K\)-Vektorraum, so nennt man diesen einen Unterraum von \((V,+,\cdot)\).
Lineare Unabhaengigkeit	Sei \(V\) ein \(K\)-Vektorraum. Die Vektoren \(v_1,\cdots,v_n \in V\) hei\ss en linear unabhaengig, wenn aus \(\sum_{i=1}^n \lambda_i v_i =0\) mit \(\lambda_1,\cdots,\lambda_n \in K\) folgt, dass \(\lambda_1,\cdots,\lambda_n \in K,\) die nicht alle gleich Null sind, so hei\ss en \(v_1,\cdots,v_n\) linear abhaengig.
Basis eines Vektorraums	Sei \(V\) ein \(K\)-Vektorraum. Eine Menge \(\{v_1,\ldots,v_n\} \subseteq V\) hei\ss t Basis von \(V\), wenn gilt: \begin{enumerate} \item \(v_1,\ldots,v_n\) sind linear unabhaengig. \item \(v_1,\ldots,v_n\) erzeugen den Vektorraum \(V\), (d.h. \(\text{Span} \{v_1,\cdots, v_n\}=V\))\end{enumerate}


Gruppeneigenschaften	Fuer jede Gruppe \((G,\oplus)\) gilt: \begin{enumerate} \item Ist \(e \in G\) ein neutrales Element und sind \(a,\tilde{a} \in G\) mit \( \tilde{a} \oplus a = e\), so gilt auch \(a \oplus \tilde{a} = e\). \item Ist \(e \in G\) ein linksneutrales Element und ist \(a \in G\), so gilt auch \[a \oplus e = a.\] \item \(G\) enthaelt genau ein neutrales Element. \item Zu jedem \(a \in G\) gibt es genau ein inverses Element. \end{enumerate}
Untergruppenkriterium	\((H, \oplus)\) ist genau dann eine Untergruppe der Gruppe \((G,\oplus)\), wenn Folgendes gilt: \begin{enumerate} \item \(\emptyset \neq H \subseteq G.\) \item \(a \oplus b \in H\) fuer alle \(a,b \in H.\) \item Fuer jedes \(a \in H\) ist sein inverses Element \(\tilde{a} \in H.\)  \end{enumerate}
Ringeigenschaften	Sei \((R,\oplus, \otimes)\) ein Ring. Dann gilt: \begin{enumerate} \item \(0 \otimes a = a \otimes 0 = 0\) fuer alle \(a \in R\) \item \(a \otimes (-b) = -(a \otimes b) = (-a) \otimes b\) und \((-a) \otimes (-b) = a \otimes b\)  fuer alle \(a,b \in R\)\end{enumerate}
Satz 3.11	Sei \((R,\oplus,\otimes)\) ein Ring mit Eins. Dann gilt: \begin{enumerate} \item Falls zu \(a \in R\) ein inverses Element (bezueglich \(\otimes\)) existiert, so ist dieses eindeutig. Man bezeichnet es dann mit \(a^{-1}\). \item Sind \(a,b \in R\) invertierbar, so ist \(a \otimes b\) invertierbar und \[(a \otimes b)^{-1}=b^{-1} \otimes a^{-1}.\]\end{enumerate}
Koerpereigenschaften	Fuer jeden Koerper \(K\) gelten folgende Aussagen: \begin{enumerate} \item \(K\) hat mindestens zwei Elemente. \item \(0 \otimes a = a \otimes 0 = 0\) fuer alle \(a \in K.\) \item \(a \otimes b = a \otimes c\) und \(a \neq 0\) impliziert \(b = c\) fuer alle, \(a,b,c \in K.\) \item \(a \otimes b = 0\) impliziert \( a = 0\) oder \(b=0\), fuer alle \(a,b \in K.\)\end{enumerate}
Unterringkriterium	Sei \((R,+,*)\) ein Ring. Eine Teilmenge \(S \subseteq R\) hei\ss t Unterring von \(R\), wenn \((S,+,*)\) ein Ring ist. Dann ist \(S\) genau dann ein Unterring von \(R\), wenn gilt: \begin{enumerate} \item \(S \subseteq R,\) \item \(0_R \in S,\) \item Fuer alle \(r,s \in S\) sind \(r+s \in S\) und \(r*s \in S\), \item Fuer jedes \(r \in S\) ist \(-r \in S\). \end{enumerate}
Teilkorperkriterium	Sei \((K,+,*)\) ein Koerper. Dann ist \((L,+,*)\) genau dann ein Teilkoerper von \((K,+,*)\), wenn gilt: \begin{enumerate} \item \(L \subseteq K,\) \item \(0_K, 1_K \in L,\) \item \(a + b \in L\) und \(a * b \in L\) fuer alle \(a,b \in L,\) \item \(-a \in L\) fuer alle \(a \in L,\) \item \(a^{-1} \in L\) fuer alle \(a \in L \setminus \{0\}.\) \end{enumerate}

Rechenregeln fuer die Matrizenmultiplikation	Fuer \(A,\hat{A} \in R^{n,m}, B, \hat{B} \in R^{m,l}\) und \(C \in R^{l,k}\) gelten: \begin{description} \item[Assoziativitaet] \(A*(B*C)=(A*B)*C\) \item[rechtsdistributiv] \((A+\hat{A})*B=A*B+\hat{A}*B\) \item[linksdistributiv] \(A*(B+\hat{B})=A*B+A*\hat{B}\) \item[to-be-named] \(I_n*A=A*I_m=A\) \end{description}
Rechenregeln fuer die skalare Matrix-Multiplikation	Fuer \(A,\hat{A} \in R^{n,m}, B, \hat{B} \in R^{m,l}\) und \(C \in R^{l,k}\) gelten: \begin{description} \item[Assoziativitaet] \((\lambda \mu)\cdot A=\lambda \cdot (\mu \cdot A)\) \item[rechtsdistributiv] \((\lambda +\mu)\cdot A = \lambda \cdot A + \mu \cdot A\) \item[linksdistributiv] \(\lambda \cdot (A+B)=\lambda \cdot A+\lambda \cdot B\) \item[to-be-named] \((\lambda \cdot A) *C=\lambda \cdot(A*C)=A*(\lambda \cdot C)\) \end{description}

Untervektorraumkriterium	Sei \((U,+,\cdot)\) ist genau dann ein Unterraum von \((V,+,\cdot)\), wenn folgende Eigenschaften gelten: \begin{enumerate} \item \(U \neq \emptyset\) \item \(v+w \in U\) fuer alle \(v,w \in U\) \item \(\lambda v \in U\) fuer alle \(\lambda \in K\) und \(v \in U\) \end{enumerate}









lineare Abbildung	Seien \(V\) und \(W\) zwei \(K\)-Vektorraeume. Eine Abbildung \(f: V \rightarrow W\) hei\ss t \(K\)-linear (kurz: linear), wenn fuer alle \(v,w \in V\) und \(\lambda \in K\) die Gleichungen \begin{enumerate} \item \(f(\lambda v)=\lambda f(v)\), \item \(f(v+w) = f(v) + f(w)\), \end{enumerate} gelten. Die Menge aller dieser Abbildungen bezeichnen wir mit \(L(V,W)\).
Homomorphismus	Eine lineare Abbildung \(f: W \rightarrow V\) wird Homomorphismus genannt.
Isomorphmismus	Ein bijektiver Homomorphismus hei\ss t Isomorphismus.
Endomorphismus	Eine Abbildung \(f \in L(V,V)\) hei\ss t Endomorphismus.
Automorphmismus	Ein bijektiver Endomorphismus hei\ss t Automorphismus.
Kern	Sind \(V\) und \(W\) zwei \(K\)-Vektorraeume und \(f \in L(V,W)\), dann hei\ss t: \[Kern(f):=\{v \in V|f(v)=0\}.\]
Bild	Sind \(V\) und \(W\) zwei \(K\)-Vektorraeume und \(f \in L(V,W)\), dann hei\ss t: \[Bild(f):=\{f(v)|v \in V\} \subseteq W.\]
Urbild	Sind \(V\) und \(W\) zwei \(K\)-Vektorraeume und \(f \in L(V,W)\). Dann definiert man das Urbild von \(w\) in \(V\) als \[f^{-1}(w):=f^{-1}({w})=\{w \in V|f(v)=w\}.\]

Darstellende Matrix eines Homomorphismus	Die durch \[(f(v_1),\ldots,f(v_m)) = (w_1,\ldots,w_n)A\] eindeutig bestimmte Matrix hei\ss t die Matrixdarstellung oder darstellende Matrix von \(f \in L(V,W)\) bzgl. der Basen \(B_1=\{v_1,\ldots,v_m\}\) von \(V\) und \(B_2=\{w_1,\ldots,w_n\}\) von \(W\). Wir bezeichnen diese Matrix mit \([f]_{B_1,B_2}\).
Koordinatenabbildung	Ist \(B=\{v_1,\ldots,v_n\}\) eine Basis des \(K\)-Vektorraums \(V\), so ist die Abbildung \[\phi_B:V \rightarrow K^{n,1}, v=\lambda_1 v_1 + \ldots + \lambda_n v_n \mapsto \phi_B(v):=\begin{bmatrix}\lambda_1\\\vdots\\\lambda_n\end{bmatrix},\] ein Isomorphismus, den wir die Koordinatenabbildung von \(V\) bzgl. der Basis \(B\) nennen.
Linearform	Ist \(V\) ein \(K\)-Vektorraum, so nennen wir eine Abbildung \(f \in L(V,K)\) eine Linearform auf \(V\).
Dualraum	Ist \(V\) ein \(K\)-Vektorraum. Den \(K\)-Vektorraum \(V^*:=L(V,K)\) nennen wir den Dualraum von \(V\).
Duale Basis	Sei \(V\) ein \(n\)-dimensionaler \(K\)-Vektorraum mit einer gegebenen Basis \(B = \{v_1,\ldots,v_n\}\). Dann gibt es genau eine Basis \(B^*=\{v_1^*,\ldots,v_n^*\}\) von \(V^*\) mit der Eigenschaft: \[v_j^*(v_j)=\delta_{ij}, \quad i,j=1,\ldots,n.\] Wir nennen \(B^*\) die zu \(B\) duale Basis \(V^*\).
Duale Abbildung	Seien \(V\) und \(W\) zwei \(K\)-Vektorraeume mit ihren jeweiligen Dualraeumen \(V^*,W^*\) und sei \(f \in L(V,W)\). Dann hei\ss t \[f^*:W^* \rightarrow V^*, \quad h \mapsto h \circ f,\] also \(f^*(h)=h \circ f\) fuer alle \(h \in W^*\), die zu \(f\) duale Abbildung.
Bilinearform	Seien \(V\) und \(W\) zwei \(K\)-Vektorraeume. Eine Abbildung \(\beta : V \times W \rightarrow K\) hei\ss t Bilinearform auf \(V \times W\), wenn \begin{enumerate} \item \(\beta (v_1 + v_2,w) = \beta (v_1,w) + \beta(v_2,w),\) \item \(\beta (v,w_1 +w_2)= \beta (v,w_1)+ \beta (v,w_2),\) \item \(\beta (\lambda v,w)=\beta (v, \lambda w)= \lambda \beta(v,w),\) \end{enumerate} fuer alle \(v,v_1,v_2 \in V, w,w_1,w_2 \in W\) und \(\lambda \in K\) gilt.
Nicht ausgeartet in der ersten Variablen	Eine Bilinearform \(\beta\) auf \(V \times W\) hei\ss t nicht ausgeartet in der ersten Variablen, wenn aus \(\beta(v,w)=0\) fuer alle \(w \in W\) folgt, dass \(v=0\) ist.
Nicht ausgeartet in der zweiten Variablen	Eine Bilinearform \(\beta\) auf \(V \times W\) hei\ss t nicht ausgeartet in der zweiten Variablen, wenn aus \(\beta(v,w)=0\) fuer alle \(v \in W\) folgt, dass \(w=0\) ist.
Nicht ausgeartete Bilinearform	Eine Bilinearform \(\beta\) auf \(V \times W\) hei\ss t nicht ausgeartete Bilinearform, falls \(\beta\) in beiden Variablen nicht ausgeartet ist.
Duales Raumpaar	Ist \(\beta\) eine nicht ausgeartete Bilinearform auf \(V \times W\), so nennen wir die Raeume \(V,W\) ein duales Paar von Raeumen oder duales Raumpaar von \(\beta\).
Symmetrische Bilinearform	Eine Bilinearform \(\beta\) auf \(V\) hei\ss t symmetrisch, wenn \(\beta (v,w)= \beta (w,v)\) fuer alle \(v,w \in V\) gilt.
Darstellende Matrix einer Bilinearform	Seien \(V\) und \(W\) zwei \(K\)-Vektorraeume mit Basen \(B_1=\{v_1,\ldots,v_m\}\) und \(B_2=\{w_1,\ldots,w_n\}\). Ist \(\beta\) eine Bilinearform auf \(V \times W\), so hei\ss t \[[\beta]_{B_1 \times B_2}=[b_{ij}] \in K^{n,m}, \quad b_{ij}:=\beta (v_j,w_i),\] die Matrixdarstellung oder darstellende Matrix von \(\beta\) bzgl. der Basen \(B_1\) und \(B_2\).
Kongruente Matrizen	Falls fuer zwei Matrizen \(A,B \in K^{n,n}\) eine Matrix \(Z \in GL_n(K)\) mit \(B=Z^T AZ\) existiert, so hei\ss en \(A\) und \(B\) kongruent.
Sesquilinearform	Seien \(V\) und \(W\) zwei \(\mathbb{C}\)-Vektorraeume. Eine Abbildung \(s: V \times W \rightarrow \mathbb{C}\) hei\ss t Sesquilinearform auf \(V \times W\), wenn \begin{enumerate} \item \(s(v_1 +v_2,w)=s(v_1,w)+s(v_2,w),\) \item \(s(\lambda v,w) = \lambda s(v,w),\) \item \(s(v,w_1+w_2)=s(v,w_1)+s(v,w_2),\) \item \(s(v,\lambda w)=\bar{\lambda} s(v,w),\) \end{enumerate} fuer alle \(v,v_1,v_2 \in V, w,w_1,w_2 \in W\) und \(\lambda \in \mathbb{C}\) gilt.
Hermitesche Sesquilinearform	Eine Sesquilinearform \(s\) auf \(V\) hei\ss t hermitesch, wenn \(s(v,w)=\overline{s(w,v)}\) fuer alle \(v, w \in V\) gilt.
Hermitesche Matrix	Ist \(A= [a_{ij}] \in \mathbb{C}^{n,m},\) so ist die hermitesch Transponierte von \(A\) die Matrix \[A^H:=[\bar{a}_{ij}]^T \in \mathbb{C}^{m,n}.\] Ist \(A=A^H\), so nennen wir \(A\) eine hermitesche Matrix.
Darstellende Matrix einer Sesquilinearform	Seien \(V\) und \(W\) zwei \(\mathbb{C}\)-Vektorraeume mit Basen \(B_1 = \{v_1,\ldots,v_m\}\) und \(B_2=\{w_1,\ldots,w_n\}\). Ist \(s\) eine Sesquilinearform auf \(V \times W\), so hei\ss t \[[s]_{B_1 \times B_2}=[b_{ij}] \in \mathbb{C}^{n,m}, \quad b_{ij}:=s(v_j,w_i),\] die Matrixdarstellung oder die darstellende Matrix von \(s\) bzgl. der Basen \(B_1\) und \(B_2\).

Skalarprodukt/inneres Produkt	Sei \(V\) ein \(K\)-Vektorraum, wobei entweder \(K=\mathbb{R}\) oder \(K=\mathbb{C}\) gelten soll. Eine Abbildung \[\langle \cdot,\cdot \rangle:V \times V \rightarrow K, \quad (v,w) \mapsto \langle v,w \rangle,\] hei\ss t Skalarprodukt oder inneres Produkt auf \(V\), wenn gilt: \begin{enumerate} \item Ist \(K = \mathbb{R}\), so ist \(\langle \cdot,\cdot \rangle\) eine symmetrische Bilinearform. Ist \(K=\mathbb{C}\), so ist \(\langle \cdot,\cdot \rangle\) eine hermitische Sesquilinearform. \item \(\langle \cdot,\cdot \rangle\) ist positiv definit, d.h. es gilt \(\langle v,v \rangle \geq 0\) fuer alle \(v \in V\) mit Gleichheit genau dann, wenn \(v=0\) ist.\end{enumerate}
Euklidischer Vektorraum	Ein \(\mathbb{R}\)-Vektorraum mit einem Skalarprodukt hei\ss t euklidischer Vektorraum.
Unitaerer Vektorraum	Ein \(\mathbb{C}\)-Vektorraum mit einem Skalarprodukt hei\ss t unitaerer Vektorraum.
Norm	Sei \(V\) ein \(K\)-Vektorraum, wobei entweder \(K=\mathbb{R}\) oder \(K=\mathbb{C}\) gelteen soll. Eine Abbildung \[\|\cdot \|:V \rightarrow \mathbb{R}, \quad v \mapsto \|v\|,\] hei\ss t Norm auf \(V\), wenn fuer alle \(v,w \in V\) und \(\lambda \in K\) gilt: \begin{enumerate} \item \(\|\lambda v\|=|\lambda| \cdot \|v\|.\) \item \(\|v\| \geq 0\) mit Gleichung genau dann, wenn \(v =0\) ist. \item \(\|v+w\| \leq \|v\| +\|w\|.\) \end{enumerate}
Normierter Raum	Ein \(K\)-Vektorraum, auf dem eine Norm definiert ist, hei\ss t normierter Raum.
Standardskalarprodukt des \(\mathbb{R}^{n,1}\)	Auf dem Vektorraum \(\mathbb{R}^{n,1}\) ist \[\langle v,w \rangle := w^Tv\] ein Skalarprodukt, welches das Standardskalarprodukt des \(\mathbb{R}^{n,1}\) genannt wird.
Standardskalarprodukt des \(\mathbb{C}^{n,1}\)	Auf dem Vektorraum \(\mathbb{C}^{n,1}\) ist \[\langle v,w \rangle := w^Hv\] ein Skalarprodukt, welches das Standardskalarprodukt des \(\mathbb{C}^{n,1}\) genannt wird.
Euklidische Norm des \(\mathbb{R}^{n,1}\)	Ist \(\langle \cdot,\cdot \rangle\) das Standardskalarprodukt des \(\mathbb{R}^{n,1}\), dann ist durch \[\|v\|:=\langle v,v \rangle ^{1/2}=(v^Tv)^{1/2}\] eine Norm definiert, die man die euklidische Norm des \(\mathbb{R}^{n,1}\) nennt.
Euklidische Norm des \(\mathbb{C}^{n,1}\)	Ist \(\langle \cdot,\cdot \rangle\) das Standardskalarprodukt des \(\mathbb{C}^{n,1}\), dann ist durch \[\|v\|:=\langle v,v \rangle ^{1/2}=(v^Hv)^{1/2}\] eine Norm definiert, die man die euklidische Norm des \(\mathbb{C}^{n,1}\) nennt.

Orthogonale Vektoren	Sei \(V\) ein euklidischer oder ein unitaerer Vektorraum mit dem Skalarprodukt \(\langle \cdot,\cdot \rangle\). Zwei Vektoren \(v,w \in V\) hei\ss en orthogonal bzgl. des gegebenen Skalarprodukts \(\langle \cdot,\cdot \rangle\), wenn \(\langle v,w \rangle =0\) gilt.
Orthogonalbasis	Eine Basis \(\{v_1,\cdots,v_j\}\) hei\ss t Orthogonalbasis, wenn \[\langle v_i,v_j \rangle =0, \quad i,j=1,\cdots,n \text{ und } i \neq j,\] gilt.
Orthonormalbasis	Eine Basis \(\{v_1,\cdots,v_j\}\) hei\ss t Orthonormalbasis, wenn \[\langle v_i,v_j \rangle =0, \quad i,j=1,\cdots,n \text{ und } i \neq j,\] gilt, sowie \[\|v_i\|=1, \quad i=1,\cdots,n,\] wobei \(\|v\|=\langle v,v \rangle ^{1/2}\) die vom Skalarprodukt induzierte Norm ist, so hei\ss t \(\{v_1,\cdots,v_j\}\) Orthogonalbasis von \(V\). Fuer eine Orthonormalbasis gilt also \(\langle v_i,v_j\rangle = \delta _{ij}.\)
Orthogonale Matrix	Eine Matrix \(Q \in \mathbb{R}^{n,n}\), deren Spalten eine Orthonormalbasis bzgl. des Standardskalarprodukts des \(\mathbb{R}^{n,1}\) bilden, hei\ss t orthogonale Matrix.
Unitaere Matrix	Eine Matrix \(Q \in \mathbb{C}^{n,n}\), deren Spalten eine Orthonormalbasis bzgl. des Standardskalarprodukts des \(\mathbb{C}^{n,1}\) bilden, hei\ss t unitaere Matrix.
Orthogonales Komplement	Sei \(V\) ein euklidischer oder ein unitaerer Vektorraum mit dem Skalarprodukt \(\langle \cdot, \cdot \rangle\) und sei \(U \subseteq V\) ein Unterraum. Dann hei\ss t \[U^{\bot}:=\{v \in V|\langle v,u \rangle =0 \text{ fuer alle } u \in U\}\] das orthogonale Komplement von \(U\) (in \(V\)).
Vektorprodukt/Kreuzprodukt	Das Vektorprodukt oder Kreuzprodukt im \(\mathbb{R}^{3,1}\) ist die Abbildung \[\mathbb{R}^{3,1} \times \mathbb{R}^{3,1} \rightarrow \mathbb{R}^{3,1} , \quad (v,w) \mapsto v \times w,\] mit \(v \times w := [v_2w_3-v_3w_2, v_3w_1-v_1w_3, v_1w_2-v_2w_1]^T\) fuer \(v=[v_1,v_2,v_2]^T, w=[w_1,w_2,w_3]^T \in \mathbb{R}^{3,1}\).

Adjungierte Abbildung	Ist \(V\) ein endlichdimensionaler euklidischer oder unitaerer Vektorraum mit dem Skalarpprodukt \(\langle \cdot,\cdot \rangle\), so gibt es zu jeder Abbildung \(f \in L(V,V)\) eine eindeutig bestimmte Abbildung \(f^{ad} \in L(V,V)\) fuer die \[\langle f(v),w \rangle = \langle v,f^{ad}(w)\rangle \quad \text{ und } \quad \langle v,f(w) \rangle = \langle f^{ad}(v),w \rangle\] fuer alle \(v,w \in V\) gilt. Dann nennt man die Abbildung \(f^{ad}\) die Adjungierte von \(f\) bzgl. \(\langle \cdot, \cdot \rangle\).
Selbstadjungierte Endomorphismus	Sei \(V\) ein endlichdimensionaler euklidischer oder unitärer Vektorraum. Eine Abbildung \(f \in L(V,V)\) hei\ss t selbstadjungiert, wenn gilt: \[f = f^{ad}\]

Eigenwert	Sei \(V\) ein \(K\)-Vektorraum und sei \(f \in L(V,V).\) Falls fuer ein \(v \in V \setminus \{0\}\)  und ein \(\lambda \in K\) die Gleichung \[f(v)=\lambda v\] gilt, so nennt man \(\lambda\) einen Eigenwert von \(f\) und \(v\) einen zum Eigenwert \(\lambda\) gehoerenden Eigenvektor von \(f\).
Eigenvektor	Sei \(V\) ein \(K\)-Vektorraum und sei \(f \in L(V,V).\) Falls fuer ein \(v \in V \setminus \{0\}\)  und ein \(\lambda \in K\) die Gleichung \[f(v)=\lambda v\] gilt, so nennt man \(\lambda\) einen Eigenwert von \(f\) und \(v\) einen zum Eigenwert \(\lambda\) gehoerenden Eigenvektor von \(f\).
Eigenraum	Sind \(V\) ein \(K\)-Vektorraum und \(\lambda \in K\) ein Eigenwert von \(f \in L(V,V)\), so hei\ss t der Unterraum \[V_f(\lambda):= Kern(\lambda Id_V - f)\] der Eigenraum von \(f\) zum Eigenwert \(\lambda\).
Geometrische Vielfachheit	Sind \(V\) ein \(K\)-Vektorraum und \(\lambda \in K\) ein Eigenwert von \(f \in L(V,V)\), so hei\ss t \[g(\lambda, f):=dim(V_f(\lambda)\] die geometrische Vielfachheit des Eigenwertes \(\lambda\) von \(f\).
\(f\)-invarianter Unterraum	Sei \(V\) ein \(K\)-Vektorraum und \(\lambda \in K\) ein Eigenwert von \(f \in L(V,V)\) und \(U \subseteq V\) ein Unterraum. Gilt \(f(U) \subseteq U\), also \(f(u) \in U\) fuer alle \(u \in U\), so hei\ss t \(U\) ein \(f\)-invarianter Unterraum von \(V\).
Charakteristisches Polynom	Sind \(n \in \mathbb{N}, V\) ein \(n\)-dimensionaler \(K\)-Vektorraum mit Basis \(B\) und \(f \in L(V,V),\) dann nennen wir \[P_f := \det(t I_n -[f]_{B,B}) \in K[t]\] das charakteristische Polynom von \(f\).
Algebraische Vielfachheit	Sei \(V\) ein endlichdimensionaler \(K\)-Vektorraum, \(f \in f(V,V)\) und sei \(\lambda \in K\) ein Eigenwert von \(f\). Hat das charakteristische Polynom die Form \[P_f = (t-\lambda)^d *g\] fuer ein \(g \in K[t]\) mit \(g(\lambda) \neq 0,\) so nennen wir \(d\) die algebraische Vielfachheit des Eigenwerts \(\lambda\) von \(f\). Wir bezeichnen diese mit \(a(\lambda,f)\).
Diagonalisierbar	Sei \(V\) ein endlichdimensionaler \(K\)-Vektorraum. Ein Endomorphismus \(f \in L(V,V)\) hei\ss t diagonalisierbar, wenn es eine Basis \(B\) von \(V\) gibt, so dass \([f]_{B,B}\) eine Diagonalmatrix ist.
Schur-Form einer Matrix	Ist \(A \in \mathbb{C}^{n,n}\), so gibt es eine unitaere Matrix \(Q \in C^{n,n}\) und eine obere Dreiecksmatrix \(R \in \mathbb{C}^{n,n}\) mit \(A=QRQ^H\). Die Matrix \(R\) nennen wir eine Schur-Form von \(A\).


Teiler	Sei \(K\) ein Koerper und \(K[t]\) der Ring der Polynome ueber \(K\). Wenn es fuer zwei Polynome \(p,s \in K[t]\) ein Polynom \(q \in K[t]\) mit \(p=s*q\) gibt, dann hei\ss t \(s\) ein Teiler von \(p\) und wir schreiben \(s|p\) (s teilt p).
teilerfremde Polynome	Sei \(K\) ein Koerper und \(K[t]\) der Ring der Polynome ueber \(K\). Zwei Polynome \(p,s \in K[t]\) hei\ss en teilerfremd, wenn aus \(q|p\) und \(q|s\) fuer ein \(q \in K[t]\) stets folgt, dass \(q\) ein kostantes Polynom ist.
Irreduzibel/Reduzibel	Sei \(K\) ein Koerper und \(K[t]\) der Ring der Polynome ueber \(K\). Ein nicht-konstantes Polynom \(p \in K[t]\) hei\ss t irreduzibel (ueber \(K\)), wenn aus \(p=s*q\) fuer zwei Polynome \(s,q \in K[t]\) folgt, dass \(s\) oder \(q\) ein konstantes Polynom ist. Falls es zwei nicht-konstante Polynome \(s,q \in K[t]\) mit \(p=s*q\) gibt, so hei\ss t \(p\) reduzibel (ueber K)
Vielfachheit	Seien \(p \in K[t]\) und \(\lambda \in K\) eine Nullstelle von \(p\). Dann ist die Vielfachheit der Nullstelle \(\lambda\) die eindeutig bestimmte natuerliche Zahl \(m\), so dass \(p=(t-\lambda )^m*q\) fuer ein Polynom \(q \in K[t]\) mit \(q(\lambda) \neq 0\) ist.

Normaler Endomorphismus	Sei \(V\) ein endlichdimensionaler euklidischer oder unitaerer Vektorraum. Ein Endomorphismus \(f \in L(V,V)\) hei\ss t normal, wenn \(f \circ f^{ad}=f^{ad} \circ f\) gilt.
Normale Matrix	 Eine Matrix \(A \in \mathbb{R}^{n,n}\) oder \(A \in \mathbb{C}^{n,n}\) hei\ss t normal, wenn \(A^TA =AA^T\) bzw. \(A^HA=AA^H\) gilt.
Traegheitsindex einer Matrix	Ist \(A \in \mathbb{R}^{n,n}\) symmetrisch oder \(A \in \mathbb{C}^{n,n}\) hermitesch mit \(n_+\) positiven, \(n_-\) negativen und \(n_0\) Null Eigenwerten, dann hei\ss t der Tripel \((n_+,n_-,n_0)\) der Traegheitsindex von \(A\).
Definitheit	Eine reelle symmetrische oder komplex hermitesche \((n \times n)\)-Matrix \(A\) hei\ss t \begin{description} \item[positiv semidefinit,]wenn \(v^HAv \geq 0\) fuer alle \(v \in \mathbb{R}^{n,1}\) bzw. \(v \in \mathbb{C}^{n,1}\) gilt, \item[positiv definit,]wenn \(v^HAv > 0\) fuer alle \(v \in \mathbb{R}^{n,1} \setminus \{0\}\) bzw. \(v \in \mathbb{C}^{n,1}\setminus \{0\}\) gilt, \item[negativ semidefinit,]wenn \(v^HAv \leq 0\) fuer alle \(v \in \mathbb{R}^{n,1}\) bzw. \(v \in \mathbb{C}^{n,1}\) gilt, \item[negativ definit,]wenn \(v^HAv < 0\) fuer alle \(v \in \mathbb{R}^{n,1} \setminus \{0\}\) bzw. \(v \in \mathbb{C}^{n,1}\setminus \{0\}\) gilt. \end{description}
